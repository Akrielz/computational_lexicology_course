{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "from project.pipeline.balance_data import balance_data_indices_reduction\n",
    "from project.pipeline.data_loader import DataLoader\n",
    "from project.task_a.train_mlp import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with open('toxic_bert_results.pickle', 'rb') as handle:\n",
    "    features = pickle.load(handle)\n",
    "    features = torch.tensor(list(features.values()))\n",
    "    features = rearrange(features, \"f n -> n f\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model_path = \"../trained_agents/feed_forward.pt\"\n",
    "mlp = build_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14000])\n"
     ]
    }
   ],
   "source": [
    "is_sexist = mlp(features)\n",
    "is_sexist = rearrange(is_sexist, \"n 1 -> n\")\n",
    "print(is_sexist.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "14000"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = DataLoader().df\n",
    "len(df_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "balanced_df_indices = balance_data_indices_reduction(df_data['label_sexist'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "target = df_data['label_sexist']\n",
    "target = torch.tensor(np.array([entry != \"not sexist\" for entry in target]))\n",
    "predictions = is_sexist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on whole dataset: 0.6895714402198792\n"
     ]
    }
   ],
   "source": [
    "metric = BinaryAccuracy()\n",
    "acc = metric(predictions, target)\n",
    "print(f\"Accuracy on whole dataset: {acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on whole dataset with prediction for everything 'non-sexist': 0.7572857141494751\n"
     ]
    }
   ],
   "source": [
    "acc = metric(torch.zeros(len(target)), target)\n",
    "print(f\"Accuracy on whole dataset with prediction for everything 'non-sexist': {acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on balanced dataset: 0.667304277420044\n"
     ]
    }
   ],
   "source": [
    "acc = metric(predictions[balanced_df_indices], target[balanced_df_indices])\n",
    "print(f\"Accuracy on balanced dataset: {acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on balanced dataset with prediction for everything 'non-sexist': 0.5\n"
     ]
    }
   ],
   "source": [
    "acc = metric(torch.zeros(len(target[balanced_df_indices])), target[balanced_df_indices])\n",
    "print(f\"Accuracy on balanced dataset with prediction for everything 'non-sexist': {acc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
